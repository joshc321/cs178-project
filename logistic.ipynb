{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Logistic Classifier for CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T10:40:43.321935Z",
     "start_time": "2023-06-08T10:40:41.529639Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from data_loader import load_training, load_testing, load_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "from Cifar10 import Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "# device configuration\n",
    "# device = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "device = torch.device('cpu')\n",
    "\n",
    "print(f'Using device {device}')\n",
    "\n",
    "# consitency in training\n",
    "torch.manual_seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T10:40:47.386251Z",
     "start_time": "2023-06-08T10:40:47.191530Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = Cifar10(train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset = Cifar10(train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hyperparameters for LogReg model:\n",
    "- solver\n",
    "- penalty\n",
    "\n",
    "Constant for all clfs:\n",
    "- C = 1/c (eliminates inversion)\n",
    "\n",
    "Three models:\n",
    "- liblinear, l1 (v1)\n",
    "- liblinear, l2 (v2)\n",
    "- sag, l2       (v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Logistic Classifier\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.log_layer = nn.Sequential(\n",
    "            nn.Linear(3 * 32 * 32, 1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        \n",
    "        x = self.log_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 3.642\n",
      "[1,  4000] loss: 3.216\n",
      "[1,  6000] loss: 3.089\n",
      "[1,  8000] loss: 3.036\n",
      "[1, 10000] loss: 2.952\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# train the model \n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward + optimze\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 35 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(testset)} test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-08T10:40:59.000782Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshcordero/miniconda3/envs/cs178/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# v1\n",
    "\n",
    "v1_tr_errors = []\n",
    "v1_te_errors = []\n",
    "v1_val_errors = []\n",
    "\n",
    "c_list = [0, 0.1, 1, 10, 50]\n",
    "\n",
    "for c in c_list:\n",
    "    if c == 0:\n",
    "        # LogReg with no penalty\n",
    "        clf = LogisticRegression(penalty=None)\n",
    "    else:\n",
    "        clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=1/c)\n",
    "\n",
    "    clf.fit(tr_X, tr_y)\n",
    "\n",
    "    acc_tr = accuracy_score(tr_y, clf.predict(tr_X))\n",
    "    acc_te = accuracy_score(te_y, clf.predict(te_X))\n",
    "    v1_tr_errors.append(acc_tr)\n",
    "    v1_te_errors.append(acc_te)\n",
    "\n",
    "plt.title(\"Varying error rates on different regularization strength on v1\")\n",
    "plt.plot(c_list, v1_tr_errors, c=\"red\", label=\"training\")\n",
    "plt.plot(c_list, v1_te_errors, c=\"green\", label=\"testing\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"reg strength\")\n",
    "plt.ylabel(\"error rate\")\n",
    "plt.show()\n",
    "\n",
    "print(v1_tr_errors)\n",
    "print(v1_te_errors)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
